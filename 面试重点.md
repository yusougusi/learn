### 先看：

#### --数据分层结构；

先从业务系统抽取数据到数据湖中，
ods临时存储层，从数据湖中获取原始的数据保存到数仓体系中，例如我们经常操作的表格有客户信息表、账户信息表、交易流水表、可疑交易报告表等；
dwd明细数据层，对数据进行清洗和转换，例如去重、异常数据的筛选、空值的替换、数据的拆分和合并等，并且对数据结构进行建模的操作，例如维度表有客户维度表、时间维度表、交易渠道维度表等，事实表有交易事实表、存取款事实表、可疑交易事实表等；

dws数据服务层，在这里会根据不同的维度，例如时间维度有天、月、季度、半年、年等，还有地域、部门、分行等维度进行原子指标的统计，例如客户交易汇总表、产品销售汇总表、风险指标汇总表等，涉及的字段统计大概有交易金额、交易笔数、交易频率、逾期率、坏账率、欺诈事件数等指标字段；

dm集市层，根据行方或者业务这边，根据他们的具体的需求进行数据的高度的汇总和统计，涉及的汇总指标有客户价值指标、销售绩效指标、风险指标、运营效率指标等，包括客户生命周期价值、客户贡献度等不同的内容，最终这里的数据会提供给应用系统进行数据的展示和其他的操作，例如报表系统或者即席查询系统等。

#### --数据建模的概念；

传统数仓的有范式建模，现在接触的hadoop大数据平台我们使用的是维度建模，维度建模又有星型模型和雪花模型，我们项目中用的是星型模型。
在星型模型中，我们会创建不同的维度宽表，将相同类型数据的维度放在一个表格中进行存储，例如产品维度表，我们会将产品的不同层级的分类、品牌、供销商等维度放在一起，为了方便后期的业务统计，不用在Hive中去进行大量的表格联合查询操作，在Hive中表连接是非常消耗性能的操作，表连接会产生非常多的shuffle的进程，影响数据统计的效率。

#### --有哪些范式：

第一范式：表格的字段是独立的不可拆分的
第二范式：表格至少有一个主键
第三范式：表格的其他字段和主键是直接相关的关系

#### --hive数据库优化操作；*****

hive中遇到性能问题是比较常见的，在sql的计算中我们会根据hive打印的日志来判断具体性能问题出现的原因。例如我们经常会根据reduce的百分比进度来判断，如果reduce百分比在中间，例如30%的位置等卡住了，这个很大的可能是服务器的资源不足导致的问题，我们可以将sql中嵌套的语句单独拿出来当成临时表来进行创建，然后再对临时表进行统计和连接，或者是表格数据很大，那么也可以对这个大的表格进行拆分；
如果reduce是卡在了99%或者100%的位置，那么很可能是发生了数据倾斜，数据倾斜又可能是因为group by的时候key值分布不均导致的，也可能是大的表格表连接导致的，如果是key值分布不均，那么首先也可以去拆分表格，对拆分的数据分别进行统计，最后去合并数据就可以了，也可以通过hive的参数来进行优化，例如groupby.skewindata等参数进行负载均衡的设置等，如果是key值里面有大量空值导致的，也可以用字符串+随机值的方式对空值进行打散的操作；
如果是大的表格导致的，可以将大的数据表拆分成多个表格进行表连接和计算的操作，也可以通过skewinjoin的参数来优化，还有mapjoin的优化器等也可以尝试使用。
数据倾斜也可以通过UDF自定义函数进行优化，我们将数据计算的部分交给python运行。

#### --oracle数据库优化的操作；

可以通过执行计划先查看sql语句运行逻辑、引用的数据库对象、资源消耗的情况包括cost还有cpu和硬盘读写的情况，还有语句执行消耗的时间等；
查看语句在运行过程中是否有用到定义的索引、索引有没有失效；
如果表格的数据量很大，还有去查看表格是否有创建合适的分区结构；
如果表格的数据特别大，也可以考虑对表格进行分表的操作；
表格中也可以修改sql语句，例如通过with as等语句优化sql；
在sql里面还有优化器，例如可以通过parallel优化器设置语句的并行等操作。

#### --常用的函数（特别是窗口函数）；

单行函数常用的有字符串的操作例如substr  instr  replace  regexp_replace  translate等，窗口函数有排名的row_number  rank  dense_rank，平移的函数有lag  lead等

#### --数据的行列转换操作；

在Hive数仓中行列转换常用的有case when、collect_list、collect_set、explode等方法，
在oracle里面有case when、pivot、unpivot等。

#### --数据ETL操作的内容（数据清洗的内容等）；

1.进行数据格式的统一，例如统一数据类型、长度、精度等
2.进行展示格式的统一，例如时间的格式统一用 yyyy-mm-dd
3.进行数据内容的筛选，例如数据的去重、异常值的筛选、乱码数据的筛选、核心字段为空的筛选、其他空值的默认值的处理、字段的合并、字段的拆分等
4.在项目中我们有一个数据开发平台（华为云平台、dataworks、星环大数据...，也有很多公司用自己开发的），在这个平台上我们会上传不同的脚本，例如我们兼容sql脚本、python、shell、其他的工具的脚本例如kettle等，在上面抽取数据进行ETL操作的时候可以去设置ETL时间、ETL的规则等，例如抽取出错的数据绝对行数或者出错的数据的比例，还可以设置出错行数到10行的时候就回滚整个操作，10个错误以内就继续运行并且保存错误的日志。

#### --做了哪些具体的工作内容，涉及的表格和指标都有哪些。

在我们的项目组中，我们是根据不同的主题域的业务线来进行工作分配的，我平时做的比较多的是客户主题和产品主题以及事件主题相关业务的工作。
我们需要负责从数据湖中抽取数据到ODS层进行数据的增量维护，例如抽取的数据源系统有银行的核心系统、贷款平台数据、移动银行的数据、客户管理系统数据、财务平台的数据、以及一些第三方金融平台的数据等。
并且在DWD中进行数据的清洗和转换，例如维度表有客户维度表、时间维度表、交易渠道维度表等，事实表有交易事实表、存取款事实表、可疑交易事实表等。
并且在DWS层进行原子指标的统计。在这里会根据不同的维度，例如时间维度有天、月、季度、半年、年等，还有地域、部门、分行等维度进行原子指标的统计，例如客户交易汇总表、产品销售汇总表、风险指标汇总表等，涉及的字段统计大概有交易金额、交易笔数、交易频率、逾期率、坏账率、欺诈事件数等指标字段。
dm集市层，根据行方或者业务这边，根据他们的具体的需求进行数据的高度的汇总和统计，涉及的汇总指标有客户价值指标、风险指标、运营效率指标等，包括客户生命周期价值、客户贡献度等不同的内容，最终这里的数据会提供给应用系统进行数据的展示和其他的操作，例如报表系统或者即席查询系统等。

### 再看：

#### --hive的分区表、分桶表、外部表、内部表；

分区表有静态分区和动态分区，我们比较常见的是用时间字段dt进行分区，如果时间的分区数据不均衡的话，那么可能在时间分区里面还会创建子分区用分行或者地域或者类型再次进行分区。
静态分区需要自己指定分区的值，并且静态可以通过load data和insert进行数据写入，动态分区需要打开动态参数和非严格模式，通过sql语句来自动划分分区的。

分桶表是通过Hash值来自动划分分桶数据的，分桶是对group by统计和join表连接进行优化的表格结构；
分区是在where阶段对数据筛选进行优化，避免进行全表扫描。

外表不能通过truncate清空数据，如果使用drop table删除表格，只会删除在mysql元数据库中的结构，不会删除在hdfs中表格的文件夹和文件。

#### --表格存储的格式和压缩方法；

在项目中有接触过textfile、parquet、sequencefile、orc等不同的存储格式，用的比较多的是textfile和orc，我们经常用到的压缩格式有gzip和zlib等，在parquet里面也有用过SNAPPY等。

#### --四个不同by的区别；

order by：对全表进行排序，不管表格的数据有多大，都是用一个reduce进行排序的，所以效率比较慢
sort by ：是在分桶中进行数据的排序的
distribute by：只能和sort by搭配使用，是用来指定分桶的字段的
cluster by：同时指定分桶字段和排序规则

#### --hive和oracle的区别；

hive的元数据库是存储在derby或者是mysql数据库中的，oracle是各种系统视图；
hive的数据是存在hdfs上的，oracle是在dbf的数据文件中；
hive是通过mr/spark/tez进行数据统计的，oracle有自己的executor计算的进程；
hive没有索引和表格的约束条件，oracle有；
Hive支持gb/tb/pb级别的运算，oracle最多只支持gb级别；
hive是高延迟的数据库，oracle是低延迟的。

#### --UDF函数的使用；

udf、udaf、udtf有什么区别？
udf是输入一行返回一行；
udaf是输入多行返回一行；
udtf是输入多行返回多行。
我们在项目中是使用python进行UDF函数的定义和使用的，一般是在数据类型和计算逻辑比较复杂的时候，例如数据有很多层的json格式的嵌套，或者是用sql编写会有很多层的数据嵌套等。

#### --python中sparksql的使用；

我们的spark是使用的spark on hive的模式；
通过python的pyspark将数据读取到python中当成一个临时表来存储，然后使用sparksql进行统计和计算将结果当成dataframe进行保存，最后再写回到hdfs中。

#### --mapreduce和spark的区别；

mapreduce是基于硬盘进行统计的，spark是基于内存，所以spark的统计效率会更高，
spark有很庞大的体系，有sparkstream/sparkrdd/sparksql等不同的体系。

#### --hdfs读写数据的流程；

写数据：
1.客户端向nn发送写入请求
2.nn校验数据是否符合规则
3.nn同意写入数据
4.客户端请求能够写入数据的dn节点
5.nn返回dn节点的信息列表
6.客户端向dn发送写入请求
7.dn返回同意写入
8.客户端开始发送block的数据
9.dn保存数据，并且在同一个机架的随机节点和另一个随机的节点进行数据的备份
10.返回写入成功

读数据：
1.客户端带着path向nn申请数据的访问
2.nn返回有这个数据的dn节点的列表
3.客户端去向dn申请数据的访问
4.dn返回同意读取
5.客户端申请读取block数据块
6.dn返回这个数据块的内容

#### --mapreduce操作数据的流程；

input读取数据
split拆分数据
map映射数据
shuffle计算数据
reduce汇总数据
finalized展示数据

### 再看：

#### --oracle索引的类型；

索引有两个大的类型分别是btree索引和bitmap索引，btree里面有主键、唯一、普通、组合、函数等，bitmap就是位图索引。

#### --索引在什么时候会失效；失效了怎么处理，怎么恢复索引；

where的时候左右的数据类型不一致，数据发生了隐性转换；
对空值进行查询，is null
取反查询，!=或者 not
模糊查询，like的数据模糊的部分在前面，例如'%xxx'
数据的统计公式和索引字段写在了一起，例如 sal+100=200
组合索引没有使用组合的第一个字段
查询的数据最终结果集的比例比较大，数据库默认使用全表扫描
在分区索引中，如果是全局索引，对分区进行了drop partition等操作，那么分区索引会失效

可以去恢复索引：alter index 索引名字 rebuild;

#### --有哪些分区的类型，各自的特点是什么？

散列分区：分区的字段没有什么特征，例如姓名、邮箱等
列表分区：分区的字段有大量的重复数据，例如性别、部门、类型、省份、城市...
范围分区：对时间、数值类型类型进行分区，例如更新时间、年龄、工资...
组合分区：上面分区的两两组合

#### --有哪些常见的系统视图？

user_tables  user_tab_columns  user_indexes user_tab_partitions  user_procedures  v$lock  dba_objects
v$sql

#### --存储过程和自定义函数的区别？

存储过程有in和out的参数，函数只有in；
存储过程可以写任意的sql语句，函数里面只能进行查询的操作；
存储过程通过call和其他的代码块进行调用的，函数只能通过dql和dml语句进行调用；
函数有一个return的返回值，存储过程没有返回值。

#### --什么是视图，优缺点是什么？

视图是一个虚拟的表格，不会占用实际的物理空间。
优点：将一个复杂sql当成一个视图的名字来使用，可以简化日常的sql语句；在传递表格数据的时候可以隐藏字段内容；可以通过添加with read only关键字来避免表格数据被修改；可以节省服务器的网络带宽。
缺点：如果使用视图进行表连接和计算，出现了性能和数据问题，会发现语句很简单但是逻辑很复杂，难以定位。

#### --执行计划里面看什么内容？

通过执行计划先查看sql语句运行逻辑、引用的数据库对象、资源消耗的情况包括cost还有cpu和硬盘读写的情况，还有语句执行消耗的时间等；